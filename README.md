# ğŸ™ï¸ Real-Time Speech Emotion Recognition with Explainability

This project implements a robust **Speech Emotion Recognition (SER)** system using deep learning techniques. It is designed for real-time prediction from live or uploaded audio. The model is based on a **CNN-BiLSTM-Attention** architecture and provides **explainability** using **Local Model-Agnostic Classification (LMAC)**, helping users understand *why* a particular emotion was predicted.

---

## ğŸ” What This Project Does

This system detects emotions such as **happy**, **sad**, **angry**, **neutral**, etc., from speech signals by:

1. **Extracting rich acoustic features** (MFCC, Mel, Chroma, Spectral Contrast, Tonnetz)
2. **Training a hybrid deep learning model** (CNN + BiLSTM + Attention)
3. **Deploying a Streamlit web app** for real-time audio input and prediction
4. **Visualizing predictions with waveform, spectrogram, and explainability**
5. **Improving interpretability using LMAC** to highlight top contributing features

---

## ğŸ“ Project Structure

finalyearproject/
â”‚
â”œâ”€â”€ My Dataset/ # Structured dataset of .wav files across emotions
â”œâ”€â”€ features/ # Preprocessed 180-dim features
â”œâ”€â”€ models/ # Trained PyTorch model weights (.pt)
â”œâ”€â”€ scaler_encoder/ # Sklearn StandardScaler and LabelEncoder
â”œâ”€â”€ screenshots/ # App UI and explanation visuals
â”œâ”€â”€ augmented_data/ # Pitch, noise, and time-augmented audio
â”œâ”€â”€ app.py # Streamlit UI with real-time prediction + explanation
â”œâ”€â”€ train_model.py # CNN-BiLSTM-Attention training loop
â”œâ”€â”€ utils.py # Helper functions for feature extraction, LMAC, plotting
â”œâ”€â”€ spectrogram_plot.py # Spectrogram drawing during recording/upload
â”œâ”€â”€ requirements.txt # All dependencies
â””â”€â”€ README.md # You're reading it!


---

## ğŸ¯ Goals Achieved

âœ… High-quality real-time SER pipeline  
âœ… Trained deep neural network with >80% accuracy  
âœ… Fully integrated Streamlit interface  
âœ… LMAC-based interpretability of predictions  
âœ… Audio visualization (waveform + spectrogram + timer)  
âœ… Data augmentation to increase robustness  
âœ… Live inference for both recorded and uploaded inputs

---

## ğŸ› ï¸ Setup Instructions

```bash
# Clone the repository
git clone https://github.com/your-username/speech-emotion-recognition.git
cd speech-emotion-recognition

# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate

# Install all dependencies
pip install -r requirements.txt

# (Optional) Train the model from scratch
python train_model.py

# Run the Streamlit app
streamlit run app.py
Model Overview
Component	Purpose
CNN Layers	Extract local patterns from temporal features (e.g., MFCC time series)
BiLSTM Layer	Capture long-range temporal dependencies across time steps
Attention	Focus on emotionally salient frames
Dense Layer	Final emotion classification (8 classes)

Input Shape: (180,)

Output Classes: ['anger', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'sarcastic', 'surprise']

ğŸ“ˆ Feature Extraction
Extracted using Librosa:

MFCC (40 coefficients)

Mel Spectrogram (40)

Chroma Features (12)

Spectral Contrast (7)

Tonnetz (6)

All features are mean-pooled over time, resulting in a 180-dimensional input vector per audio file.

ğŸ”„ Data Augmentation Techniques
To improve generalization and reduce overfitting:

âœ… Pitch shifting

âœ… Time stretching

âœ… Noise injection

Each raw audio sample is transformed into multiple variations to simulate real-world variability.

ğŸ§ª Training Details
Parameter	Value
Optimizer	Adam
Learning Rate	0.0005
Batch Size	32
Epochs	100
Loss Function	CrossEntropy
Validation Split	20%

The model is trained with live training and validation accuracy/loss plots to monitor convergence.

ğŸ“Š Evaluation
âœ… Accuracy > 80% on validation set

âœ… Confusion matrix analysis

âœ… Per-class accuracy visualization

âœ… LMAC explanations for every prediction

âœ… Confidence score shown in real-time interface

ğŸ” Explainability with LMAC
Local Model-Agnostic Classification (LMAC) explains predictions by:

Perturbing inputs and observing model changes

Ranking top contributing features (MFCC, Mel, etc.)

Displaying a bar chart showing feature importance per prediction

Helps users understand why the model predicted a specific emotion.

ğŸŒ Streamlit Interface Features
ğŸ™ï¸ Start Recording: Capture 10 seconds of audio

ğŸ“‚ Upload Audio: Use existing .wav file

ğŸ“‰ Real-time Spectrogram + Waveform

ğŸ“Š Predicted Emotion + Confidence

ğŸ§  LMAC bar graph for interpretability

â³ Countdown timer during recording

ğŸ“· Sample Visuals
Spectrogram	LMAC Explainability

ğŸ“Œ Future Improvements
Use transformer-based models (e.g., Wav2Vec 2.0)

Add emotion localization within audio

Create a mobile version of the app

Add speaker recognition and emotion intensity scoring

ğŸ‘ Acknowledgments
ğŸ“˜ LMAC - Francesco Paissan

ğŸ”§ Librosa, PyTorch, Streamlit, Scikit-learn

ğŸ“œ License
This project is licensed under the MIT License. Feel free to fork and contribute!
